# -*- coding: utf-8 -*-
"""ML_HW1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/148h0oq6hAk4hdYoVMSeYY7FIaPifKRir
"""

import numpy as np
from scipy.linalg import norm
from sklearn import datasets, svm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Problem 1(a)
x1 = np.transpose([2, 3, 4, 8, 9])
x2 = np.transpose([2, -3, -4, 89])
print("Problem 1a sol: ", len(x1), len(x2))

# Problem 1(b)
    # L1 values
x1_L1 = sum(abs(x1))
x2_L1 = sum(abs(x2))
    # L2 values
x1_L2 = np.sqrt(sum(abs(x1**2)))
x2_L2 = np.sqrt(sum(abs(x2**2)))
    # Linf values
x1_Linf = max(abs(x1))
x2_Linf = max(abs(x2))
print("Problem 1b sol: ",x1_L2, x2_L2, x1_Linf, x2_Linf)

# Problem 3
W = [[1, -1], [2, 0]]
norm_l1 = norm(W, 1)
norm_l2 = norm(W, 2)
norm_linf = norm(W, 'fro')
print("Problem 3 sol: ", norm_l1, norm_l2, norm_linf)

# Problem 4
iris = datasets.load_iris()
X = iris.data[:, :3]
y = iris.target

X_train, X_test, y_train, y_test=train_test_split(
    X,y,
    test_size=0.40,
    train_size=0.60,
    random_state=123,
    shuffle=True,
    stratify=y)

clf = svm.SVC()
clf.fit(X_train, y_train)
preds = clf.predict(X_test)
print(preds)
print('actual iris species')
print(y_test)
acc = accuracy_score(y_test,clf.predict(X_test) )
print('Problem 4 sol: ', acc)

# Problem 5
import tensorflow as tf
from numpy import loadtxt
from keras.models import Sequential
from keras.layers import Dense
from keras import optimizers
from tensorflow.python.keras.optimizers import *
import numpy as np


#Load dataset
#split into input (X) and output (y)
X = np.array([[1], [2], [3], [4],[5], [6], [7], [8],[9], [10], [11], [12], [13],[14], [15], [16], [17],[18], [19], [20],
              [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40],
              [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60],
               [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80],
                [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100]])
X = X*1.0
y = np.array([[1], [1.1041], [1.1699], [1.2190],[1.2585], [1.2917], [1.3205], [1.3459],[1.3687], [1.3895], [1.4085], [1.4262], [1.4426],[1.4579], [1.4724], [1.4859], [1.4989],[1.5112], [1.5229], [1.5341],
              [1.5449], [1.5552], [1.5651], [1.5746], [1.5838], [1.5927], [1.6013], [1.6097], [1.6178], [1.6256], [1.6333], [1.6407], [1.6479], [1.6549], [1.6618], [1.6685], [1.6751], [1.6815], [1.6877], [1.6938],
              [1.6998], [1.7057], [1.7114], [1.7170], [1.7226], [1.7280], [1.7333], [1.7385], [1.7436], [1.7487], [1.7536], [1.7585], [1.7633], [1.7680], [1.7727], [1.7772], [1.7817], [1.7862], [1.7905], [1.7948],
               [1.7991], [1.8033], [1.8074], [1.8115], [1.8155], [1.8194], [1.8233], [1.8272], [1.8310], [1.8348], [1.8385], [1.8422], [1.8458], [1.8494], [1.8530], [1.8565], [1.8599], [1.8634], [1.8668], [1.8701],
                [1.8734], [1.8767], [1.8799], [1.8832], [1.8864], [1.8895], [1.8927], [1.8958], [1.8988], [1.9019], [1.9049], [1.9078], [1.9108], [1.9137], [1.9166], [1.9195], [1.9223], [1.9251], [1.9279], [1.9307]])
y = y*1.0

#define keras model
model = Sequential()

model.add(Dense(6,input_dim=1,activation='relu'))
model.add(Dense(6,activation='relu'))
model.add(Dense(6,activation='relu'))
model.add(Dense(1))

#compile the keras model
opt = optimizers.Adam(learning_rate=0.001)
mse = tf.keras.losses.MeanSquaredError(
    reduction=tf.keras.losses.Reduction.SUM)
model.compile(loss=mse, optimizer=opt)



#fit the keras model on the dataset (CPU)
model.fit(X,y,epochs=2000,batch_size=10, verbose=0)
model.summary()

#make class predictions with the model
predictions = model.predict(X)

#summarize the first 10 cases
for i in range(10):
    print('%s => %.2f (expected %.2f)' %(X[i].tolist(), predictions[i], y[i]) )

import matplotlib.pyplot as plt
number_grid = np.linspace(1, 100, 100)
plt.scatter(X,y, label='data')
plt.plot(number_grid,model.predict(np.expand_dims(number_grid,axis=1)) , color='red', label='model')
plt.xlabel('number')
plt.ylabel('square root')
plt.legend()

# Problem 6
import tensorflow as tf
from numpy import loadtxt
from keras.models import Sequential
from keras.layers import Dense
from keras import optimizers
from tensorflow.python.keras.optimizers import *
import numpy as np
from sklearn import datasets

iris = datasets.load_iris()
iris.data
iris.feature_names
iris.target_names

#Load dataset
#split into input (X) and output (y)
X = np.array(iris.data[:, 0]) #Input sepal length
X = X*1.0
y = np.array(iris.data[:, 2]) #output petal length
y = y*1.0

#define keras model
model = Sequential()

model.add(Dense(6,input_dim=1,activation='relu'))
model.add(Dense(6,activation='relu'))
model.add(Dense(6,activation='relu'))
model.add(Dense(6,activation='relu'))

model.add(Dense(1))

#compile the keras model
opt = optimizers.Adam(learning_rate=0.001)
mse = tf.keras.losses.MeanSquaredError(
    reduction=tf.keras.losses.Reduction.SUM)
model.compile(loss=mse, optimizer=opt)

#fit the keras model on the dataset (CPU)
model.fit(X,y,epochs=2000,batch_size=10, verbose=0)
model.summary()

#make class predictions with the model
predictions = model.predict(X)

#summarize the first 10 cases
for i in range(10):
    print('%s => %.2f (expected %.2f)' %(X[i].tolist(), predictions[i], y[i]) )

import matplotlib.pyplot as plt
#number_grid = np.linspace(1, 100, 100)
plt.scatter(X,y, label='data')
plt.scatter(X, predictions, label='model')
#plt.plot(number_grid,model.predict(np.expand_dims(number_grid,axis=1)) , color='red', label='model')
plt.xlabel('Sepal Length')
plt.ylabel('Petal Length')
plt.legend()